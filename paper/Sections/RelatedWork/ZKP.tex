\subsection{Privacy Preserving and Verifiable Machine Learning}

Privacy preserving evaluation of machine learning models has become of particular interest given the changes in regulations (maybe cite GDPR) or events increasing the general public awareness on how private data is used to track users (cite something). \owncomment[IQ]{Maybe this could be motivated in the introduction}
Several approaches are present in current literature. On the one hand, we have Homomorphic Encryption based schemes \cite{Dowlin:2016:CAN:3045390.3045413,mlconfidential,bos2013private}, where the user encrypts the data over which the model has to be evaluated and sends it to the server. Then the server evaluates over the encrypted data and sends back the result to the user. This method is both private and verifiable, as the server never gets to see the plain user data, but is the evaluating the model, and hence is convinced of the validity of the output of the computation. 
However, such schemes centralise the evaluation of ML models, which can become problematic when a high number of requests are received. \owncomment[IQ]{Read the evaluations on the referenced papers to try and make a point here.}
Moreover, evaluating ML models over encrypted data gives more restrictions than tha ZK case, as non-linear and non-polynomial functions cannot be computed (limiting like that the application of several models as random forests and forcing an approximation to linear functions in many other such as logistic regression or (D)NN). \owncomment[IQ]{Here would be cool some references and numbers on how this affects accuracy.}

Another approach to offer privacy preserving machine learning is to evaluate the model locally, avoiding data to be sent to the server. However, if, unlike \name, such approach is taken without proving correct evaluation of the model \cite{DBLP:journals/corr/abs-1710-03275,Bilenko:2011:PCP:2020408.2020475,Guha:2011:PPP:1972457.1972475}, verification is lost. In these papers the model is evaluated for targeted advertising, which can be argued that users are interested in evaluating the latter correctly, removing like that the need of verifying the correct evaluation. However, in other cases (such as bot detection) the user's interest might be of faking the evaluation model, and therefore such limits open the gap for user attacks. 

To the best of our knowledge, the only papers that aim at solving this problem with provable machine learning evaluated locally without a trusted execution environment are the ones presented by Davidson, Fredrikson and Livshits \citep{Davidson:2014:MMO:2664243.2664266} and by Danezis \textit{et al.} \cite{Danezis:2012:PCP:2359015.2359018}. The first paper tries to solve a similar problem, where personalization of a user device is done by evaluating a model locally on the user's machine. This work uses Bayesian classification, for which they need from 100-300 feature words. The generation of correct model evaluation for such range of feature words ranges from 30 to 80 seconds. Moreover, this study uses standard techniques for constructing zero knowledge proofs, which give a big overhead to the verifier. For our particular use case (where the verifier needs to handle several requests simultaneously), such an overhead for the verifier is not acceptable. 

The second paper tackling the problem with verifiable evaluation of ML models \cite{Danezis:2012:PCP:2359015.2359018} presents a similar approach as the one presented in this paper. It proposes a solution where after the evaluation of  Random Forest and Hidden Markov models, the user generates a zero knowledge proof of correct evaluation. However, this paper misses an evaluation study or availability of the code, which makes a study of the scalability of their approach inaccessible. Moreover, as in \cite{Davidson:2014:MMO:2664243.2664266}, the zero knowledge proofs give a big overhead to the verifier. 